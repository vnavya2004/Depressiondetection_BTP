* mix(multicased,roberta(lstm)) file contains 5 datasets results using Bert(multicased) and 4 datasets results using roberta model. 
* SpanishRoberta file contains Spanish Roberta model results.
* Bangla(multilingual) file have results of bangla dataset using huggingface multilingual model.
* llama2(multilingual)arabic and korean datasets results(preprocessing using generative ai and multilingual model for training)
* llama2(multilingual)bangla,japaneese results( same as above)
* llama2(multilingual)spanish results( same as above)
Total =11+5=16.
results as below:
Datasets	      Model_Used	      Accuracy	precision	Recall	F1 score
1.Arabic Dataset	Multi-cased BERT	0.7688	0.772727	0.7616	0.7671
2.Bangla Dataset	Multi-cased BERT	0.60878	0.35934	0.7113	0.477489
3.Japaneese Dataset	Multi-cased BERT	0.788	0.76086	0.84	0.7984
4.Korean Dataset	Multi-cased BERT	0.86	0.887	0.824	0.8547
5.Spanish Dataset	Multi-cased BERT	0.839	0.8234	0.875	0.8488
6.Arabic Dataset	XLM_ROBERTA	0.7916	0.8113	0.76	0.7848
7.Bangla Dataset	XLM_ROBERTA	0.7732	0.5952	0.3049	0.4032
8.Japaneese Dataset	XLM_ROBERTA	0.548	0.5455	0.576	0.5603
9.Korean Dataset	XLM_ROBERTA	0.604	0.565	0.904	0.6954
10.Spanish Dataset	XLM_ROBERTA	0.8704	0.8559	0.9002	0.8775
11.Bangla Dataset	Multi-Lingual BERT(pytorch)	0.848825	0.8492	0.8488	0.8486

LLAMA2 PREPROCESSING AND USE MULTILINGUAL BERT MODEL 

12.Arabic Dataset	Multi-lingual BERT	0.9764	0.976472	0.9764	0.97639915
13.Bangla Dataset	Multi-lingual BERT	0.774259448	0.780243061	0.774259448	0.771513741
14.Japaneese Dataset	Multi-lingual BERT	0.988	0.988288	0.988	0.987998272
14.Korean Dataset	Multi-lingual BERT	0.964	0.965568	0.964	0.963971754
16.Spanish Dataset	Multi-lingual BERT 0.913076557299096 0.913741848494714 0.913076557299096 0.913005724789823				
